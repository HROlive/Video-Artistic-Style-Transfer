{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iygZP03eOymf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZAuc2thwQuvO",
    "outputId": "9793db3d-f7ed-494a-a99f-7715293df276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/CS50_Final_Project/Video_Artistic_Style_Transfer\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/CS50_Final_Project/Video_Artistic_Style_Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_sDghy4RpPj"
   },
   "outputs": [],
   "source": [
    "# google colab does not come with torch installed\n",
    "\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NTdz1AOVR-rE",
    "outputId": "ae1cc1e6-6751-4371-c978-a5e411b79c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# we will verify that GPU is enabled for this notebook\n",
    "# following should print: CUDA is available!  Training on GPU ...\n",
    "# \n",
    "# if it prints otherwise, then you need to enable GPU: \n",
    "# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "aSgAs6p2TjcP",
    "outputId": "d24af108-ad27-405a-ea5f-c9d11dedfcef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling Pillow-5.3.0:\n",
      "  Successfully uninstalled Pillow-5.3.0\n",
      "Collecting Pillow==5.3.0\n",
      "  Using cached https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-5.3.0\n",
      "5.3.0\n"
     ]
    }
   ],
   "source": [
    "# we need pillow version of 5.3.0\n",
    "# we will uninstall the older version first\n",
    "!pip uninstall -y Pillow\n",
    "# install the new one\n",
    "!pip install Pillow==5.3.0\n",
    "# import the new one\n",
    "import PIL\n",
    "print(PIL.PILLOW_VERSION)\n",
    "# this should print 5.3.0. If it doesn't, then restart your runtime:\n",
    "# Menu > Runtime > Restart Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtMBAFjRN2II"
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages:\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image  # Python Image Library for image processing\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import numpy as np  # Numerical computation\n",
    "import torch  # Neural network computation\n",
    "from torch import optim  # optimizer to minimize the loss function\n",
    "from torchvision import transforms, models  # Transformations on images and pre-trained models\n",
    "import os, os.path  # To count the number of image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZpVE0dcjN5TJ",
    "outputId": "5c40da84-36e2-42d2-a6d7-40efa5c4a1f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.torch/models/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 574673361/574673361 [00:06<00:00, 82195845.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# LOAD VGG19 (features only):\n",
    "# vgg19.features \t: It consists of all the convolutional and pooling layers\n",
    "# vgg19.classifier \t: It consists of the 3 linear classifier layers at the end\n",
    "\n",
    "# We load in the pre-trained model and freeze the weights:\n",
    "styleTransferModel = models.vgg19(pretrained=True).features\n",
    "\n",
    "# Freeze all the VGG parameters since we are only optimizing the target image:\n",
    "for params in styleTransferModel.parameters():\n",
    "\tparams.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "br-SbR_BN_S9",
    "outputId": "eb28c497-70fa-432d-c8cf-e7eb9aab0b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Move the model to the GPU if available:\n",
    "styleTransferModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyo-yDUJOBTF"
   },
   "outputs": [],
   "source": [
    "# Load the content and style images:\n",
    "def load_image(image_path, max_size=400, shape=None):\n",
    "    # Load in an image and make sure that it is <= 400 pixels in the X-Y dimension:\n",
    "    # Convert the image to RGB:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Resize the image as a large image will slow down processing:\n",
    "    if max(image.size) > max_size:\n",
    "        img_size = max_size\n",
    "    else:\n",
    "        img_size = max(image.size)\n",
    "\n",
    "    if shape is not None:\n",
    "        img_size = shape\n",
    "\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    # Discard the transparent alpha channel (that's the :3) and add the batch dimension:\n",
    "    image = img_transform(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZQJkUfLOEzW"
   },
   "outputs": [],
   "source": [
    "# CONTENT AND STYLE FEATURES:\n",
    "# Map the layer names to the names given in the paper:\n",
    "def get_features(image, model, layers=None):\n",
    "    # Run an image forward through a model and get the features for a set of layers:\n",
    "\n",
    "    # Layers for the content and style representation of an image:\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1',\n",
    "                  '10': 'conv3_1',\n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  # Content representation\n",
    "                  '28': 'conv5_1',}\n",
    "\n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model:\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umMxhudCOHbc"
   },
   "outputs": [],
   "source": [
    "# GRAM MATRIX:\n",
    "# Define the gram matrix of the tensor:\n",
    "def gram_matrix(tensor):\n",
    "    # Calculate the Gram Matrix of a given tensor:\n",
    "    # Get the batch_size, depth, height and width of the image:\n",
    "    batch_size, depth, height, width = tensor.size()\n",
    "    #(Sample tensor shape : torch.Size([1, 64, 400, 592]))\n",
    "    # Here, batch_size = 1, depth = 64, height = 400, width = 592\n",
    "    \n",
    "    # Vectorize the input image tensor and add all the feature maps:\n",
    "    tensor = tensor.view(depth, height * width)\n",
    "    # Transpose the image tensor:\n",
    "    tensor_t = tensor.t()\n",
    "    # Compute the gram matrix by multiplying the matrix by its transpose:\n",
    "    gram = torch.mm(tensor, tensor_t)\n",
    "\n",
    "    # Return the gram matrix:\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apdv4c_gOJh8"
   },
   "outputs": [],
   "source": [
    "def convert_video_to_frames(video_directory):\n",
    "    # Load the input video:\n",
    "    capture_video = cv2.VideoCapture(video_directory)\n",
    "    # Read the input frame:\n",
    "    success, frame = capture_video.read()\n",
    "    # Set counter for number of frames read:\n",
    "    count = 1\n",
    "\n",
    "    while success:\n",
    "        # Save the frame that is read:\n",
    "        cv2.imwrite('input_content_frames/frame_%d.jpg' % count, frame)\n",
    "        # Read the next input frame:\n",
    "        success, frame = capture_video.read()\n",
    "        print(\"Read a new frame: \", count, \"Success: \", success)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4250
    },
    "colab_type": "code",
    "id": "tXY8GSCdONr0",
    "outputId": "a1d4b143-8d93-4d0b-adb5-7cae295e00a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read a new frame:  1 Success:  True\n",
      "Read a new frame:  2 Success:  True\n",
      "Read a new frame:  3 Success:  True\n",
      "Read a new frame:  4 Success:  True\n",
      "Read a new frame:  5 Success:  True\n",
      "Read a new frame:  6 Success:  True\n",
      "Read a new frame:  7 Success:  True\n",
      "Read a new frame:  8 Success:  True\n",
      "Read a new frame:  9 Success:  True\n",
      "Read a new frame:  10 Success:  True\n",
      "Read a new frame:  11 Success:  True\n",
      "Read a new frame:  12 Success:  True\n",
      "Read a new frame:  13 Success:  True\n",
      "Read a new frame:  14 Success:  True\n",
      "Read a new frame:  15 Success:  True\n",
      "Read a new frame:  16 Success:  True\n",
      "Read a new frame:  17 Success:  True\n",
      "Read a new frame:  18 Success:  True\n",
      "Read a new frame:  19 Success:  True\n",
      "Read a new frame:  20 Success:  True\n",
      "Read a new frame:  21 Success:  True\n",
      "Read a new frame:  22 Success:  True\n",
      "Read a new frame:  23 Success:  True\n",
      "Read a new frame:  24 Success:  True\n",
      "Read a new frame:  25 Success:  True\n",
      "Read a new frame:  26 Success:  True\n",
      "Read a new frame:  27 Success:  True\n",
      "Read a new frame:  28 Success:  True\n",
      "Read a new frame:  29 Success:  True\n",
      "Read a new frame:  30 Success:  True\n",
      "Read a new frame:  31 Success:  True\n",
      "Read a new frame:  32 Success:  True\n",
      "Read a new frame:  33 Success:  True\n",
      "Read a new frame:  34 Success:  True\n",
      "Read a new frame:  35 Success:  True\n",
      "Read a new frame:  36 Success:  True\n",
      "Read a new frame:  37 Success:  True\n",
      "Read a new frame:  38 Success:  True\n",
      "Read a new frame:  39 Success:  True\n",
      "Read a new frame:  40 Success:  True\n",
      "Read a new frame:  41 Success:  True\n",
      "Read a new frame:  42 Success:  True\n",
      "Read a new frame:  43 Success:  True\n",
      "Read a new frame:  44 Success:  True\n",
      "Read a new frame:  45 Success:  True\n",
      "Read a new frame:  46 Success:  True\n",
      "Read a new frame:  47 Success:  True\n",
      "Read a new frame:  48 Success:  True\n",
      "Read a new frame:  49 Success:  True\n",
      "Read a new frame:  50 Success:  True\n",
      "Read a new frame:  51 Success:  True\n",
      "Read a new frame:  52 Success:  True\n",
      "Read a new frame:  53 Success:  True\n",
      "Read a new frame:  54 Success:  True\n",
      "Read a new frame:  55 Success:  True\n",
      "Read a new frame:  56 Success:  True\n",
      "Read a new frame:  57 Success:  True\n",
      "Read a new frame:  58 Success:  True\n",
      "Read a new frame:  59 Success:  True\n",
      "Read a new frame:  60 Success:  True\n",
      "Read a new frame:  61 Success:  True\n",
      "Read a new frame:  62 Success:  True\n",
      "Read a new frame:  63 Success:  True\n",
      "Read a new frame:  64 Success:  True\n",
      "Read a new frame:  65 Success:  True\n",
      "Read a new frame:  66 Success:  True\n",
      "Read a new frame:  67 Success:  True\n",
      "Read a new frame:  68 Success:  True\n",
      "Read a new frame:  69 Success:  True\n",
      "Read a new frame:  70 Success:  True\n",
      "Read a new frame:  71 Success:  True\n",
      "Read a new frame:  72 Success:  True\n",
      "Read a new frame:  73 Success:  True\n",
      "Read a new frame:  74 Success:  True\n",
      "Read a new frame:  75 Success:  True\n",
      "Read a new frame:  76 Success:  True\n",
      "Read a new frame:  77 Success:  True\n",
      "Read a new frame:  78 Success:  True\n",
      "Read a new frame:  79 Success:  True\n",
      "Read a new frame:  80 Success:  True\n",
      "Read a new frame:  81 Success:  True\n",
      "Read a new frame:  82 Success:  True\n",
      "Read a new frame:  83 Success:  True\n",
      "Read a new frame:  84 Success:  True\n",
      "Read a new frame:  85 Success:  True\n",
      "Read a new frame:  86 Success:  True\n",
      "Read a new frame:  87 Success:  True\n",
      "Read a new frame:  88 Success:  True\n",
      "Read a new frame:  89 Success:  True\n",
      "Read a new frame:  90 Success:  True\n",
      "Read a new frame:  91 Success:  True\n",
      "Read a new frame:  92 Success:  True\n",
      "Read a new frame:  93 Success:  True\n",
      "Read a new frame:  94 Success:  True\n",
      "Read a new frame:  95 Success:  True\n",
      "Read a new frame:  96 Success:  True\n",
      "Read a new frame:  97 Success:  True\n",
      "Read a new frame:  98 Success:  True\n",
      "Read a new frame:  99 Success:  True\n",
      "Read a new frame:  100 Success:  True\n",
      "Read a new frame:  101 Success:  True\n",
      "Read a new frame:  102 Success:  True\n",
      "Read a new frame:  103 Success:  True\n",
      "Read a new frame:  104 Success:  True\n",
      "Read a new frame:  105 Success:  True\n",
      "Read a new frame:  106 Success:  True\n",
      "Read a new frame:  107 Success:  True\n",
      "Read a new frame:  108 Success:  True\n",
      "Read a new frame:  109 Success:  True\n",
      "Read a new frame:  110 Success:  True\n",
      "Read a new frame:  111 Success:  True\n",
      "Read a new frame:  112 Success:  True\n",
      "Read a new frame:  113 Success:  True\n",
      "Read a new frame:  114 Success:  True\n",
      "Read a new frame:  115 Success:  True\n",
      "Read a new frame:  116 Success:  True\n",
      "Read a new frame:  117 Success:  True\n",
      "Read a new frame:  118 Success:  True\n",
      "Read a new frame:  119 Success:  True\n",
      "Read a new frame:  120 Success:  True\n",
      "Read a new frame:  121 Success:  True\n",
      "Read a new frame:  122 Success:  True\n",
      "Read a new frame:  123 Success:  True\n",
      "Read a new frame:  124 Success:  True\n",
      "Read a new frame:  125 Success:  True\n",
      "Read a new frame:  126 Success:  True\n",
      "Read a new frame:  127 Success:  True\n",
      "Read a new frame:  128 Success:  True\n",
      "Read a new frame:  129 Success:  True\n",
      "Read a new frame:  130 Success:  True\n",
      "Read a new frame:  131 Success:  True\n",
      "Read a new frame:  132 Success:  True\n",
      "Read a new frame:  133 Success:  True\n",
      "Read a new frame:  134 Success:  True\n",
      "Read a new frame:  135 Success:  True\n",
      "Read a new frame:  136 Success:  True\n",
      "Read a new frame:  137 Success:  True\n",
      "Read a new frame:  138 Success:  True\n",
      "Read a new frame:  139 Success:  True\n",
      "Read a new frame:  140 Success:  True\n",
      "Read a new frame:  141 Success:  True\n",
      "Read a new frame:  142 Success:  True\n",
      "Read a new frame:  143 Success:  True\n",
      "Read a new frame:  144 Success:  True\n",
      "Read a new frame:  145 Success:  True\n",
      "Read a new frame:  146 Success:  True\n",
      "Read a new frame:  147 Success:  True\n",
      "Read a new frame:  148 Success:  True\n",
      "Read a new frame:  149 Success:  True\n",
      "Read a new frame:  150 Success:  True\n",
      "Read a new frame:  151 Success:  True\n",
      "Read a new frame:  152 Success:  True\n",
      "Read a new frame:  153 Success:  True\n",
      "Read a new frame:  154 Success:  True\n",
      "Read a new frame:  155 Success:  True\n",
      "Read a new frame:  156 Success:  True\n",
      "Read a new frame:  157 Success:  True\n",
      "Read a new frame:  158 Success:  True\n",
      "Read a new frame:  159 Success:  True\n",
      "Read a new frame:  160 Success:  True\n",
      "Read a new frame:  161 Success:  True\n",
      "Read a new frame:  162 Success:  True\n",
      "Read a new frame:  163 Success:  True\n",
      "Read a new frame:  164 Success:  True\n",
      "Read a new frame:  165 Success:  True\n",
      "Read a new frame:  166 Success:  True\n",
      "Read a new frame:  167 Success:  True\n",
      "Read a new frame:  168 Success:  True\n",
      "Read a new frame:  169 Success:  True\n",
      "Read a new frame:  170 Success:  True\n",
      "Read a new frame:  171 Success:  True\n",
      "Read a new frame:  172 Success:  True\n",
      "Read a new frame:  173 Success:  True\n",
      "Read a new frame:  174 Success:  True\n",
      "Read a new frame:  175 Success:  True\n",
      "Read a new frame:  176 Success:  True\n",
      "Read a new frame:  177 Success:  True\n",
      "Read a new frame:  178 Success:  True\n",
      "Read a new frame:  179 Success:  True\n",
      "Read a new frame:  180 Success:  True\n",
      "Read a new frame:  181 Success:  True\n",
      "Read a new frame:  182 Success:  True\n",
      "Read a new frame:  183 Success:  True\n",
      "Read a new frame:  184 Success:  True\n",
      "Read a new frame:  185 Success:  True\n",
      "Read a new frame:  186 Success:  True\n",
      "Read a new frame:  187 Success:  True\n",
      "Read a new frame:  188 Success:  True\n",
      "Read a new frame:  189 Success:  True\n",
      "Read a new frame:  190 Success:  True\n",
      "Read a new frame:  191 Success:  True\n",
      "Read a new frame:  192 Success:  True\n",
      "Read a new frame:  193 Success:  True\n",
      "Read a new frame:  194 Success:  True\n",
      "Read a new frame:  195 Success:  True\n",
      "Read a new frame:  196 Success:  True\n",
      "Read a new frame:  197 Success:  True\n",
      "Read a new frame:  198 Success:  True\n",
      "Read a new frame:  199 Success:  True\n",
      "Read a new frame:  200 Success:  True\n",
      "Read a new frame:  201 Success:  True\n",
      "Read a new frame:  202 Success:  True\n",
      "Read a new frame:  203 Success:  True\n",
      "Read a new frame:  204 Success:  True\n",
      "Read a new frame:  205 Success:  True\n",
      "Read a new frame:  206 Success:  True\n",
      "Read a new frame:  207 Success:  True\n",
      "Read a new frame:  208 Success:  True\n",
      "Read a new frame:  209 Success:  True\n",
      "Read a new frame:  210 Success:  True\n",
      "Read a new frame:  211 Success:  True\n",
      "Read a new frame:  212 Success:  True\n",
      "Read a new frame:  213 Success:  True\n",
      "Read a new frame:  214 Success:  True\n",
      "Read a new frame:  215 Success:  True\n",
      "Read a new frame:  216 Success:  True\n",
      "Read a new frame:  217 Success:  True\n",
      "Read a new frame:  218 Success:  True\n",
      "Read a new frame:  219 Success:  True\n",
      "Read a new frame:  220 Success:  True\n",
      "Read a new frame:  221 Success:  True\n",
      "Read a new frame:  222 Success:  True\n",
      "Read a new frame:  223 Success:  True\n",
      "Read a new frame:  224 Success:  True\n",
      "Read a new frame:  225 Success:  True\n",
      "Read a new frame:  226 Success:  True\n",
      "Read a new frame:  227 Success:  True\n",
      "Read a new frame:  228 Success:  True\n",
      "Read a new frame:  229 Success:  True\n",
      "Read a new frame:  230 Success:  True\n",
      "Read a new frame:  231 Success:  True\n",
      "Read a new frame:  232 Success:  True\n",
      "Read a new frame:  233 Success:  True\n",
      "Read a new frame:  234 Success:  True\n",
      "Read a new frame:  235 Success:  True\n",
      "Read a new frame:  236 Success:  True\n",
      "Read a new frame:  237 Success:  True\n",
      "Read a new frame:  238 Success:  True\n",
      "Read a new frame:  239 Success:  True\n",
      "Read a new frame:  240 Success:  True\n",
      "Read a new frame:  241 Success:  True\n",
      "Read a new frame:  242 Success:  True\n",
      "Read a new frame:  243 Success:  True\n",
      "Read a new frame:  244 Success:  True\n",
      "Read a new frame:  245 Success:  True\n",
      "Read a new frame:  246 Success:  True\n",
      "Read a new frame:  247 Success:  True\n",
      "Read a new frame:  248 Success:  True\n",
      "Read a new frame:  249 Success:  False\n"
     ]
    }
   ],
   "source": [
    "# Split the video into frames:\n",
    "convert_video_to_frames('input_video/sample_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3th2rfaYOPqt"
   },
   "outputs": [],
   "source": [
    "# Function to un-normalize an image and convert from a Tensor image to a NumPy image for display or writing to disk:\n",
    "def img_convert(tensor):\n",
    "    # Display a tensor as an image:\n",
    "\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ag1fOcQORyl"
   },
   "outputs": [],
   "source": [
    "def apply_style_transfer(content_img_dir, style_img_dir):\n",
    "    content_image_count = 1\n",
    "    style_image_count = 1\n",
    "\n",
    "    # Retrieve the total number of content image frames and style frames:\n",
    "    num_content_imgs = len([name for name in os.listdir(content_img_dir) if os.path.isfile(os.path.join(content_img_dir, name))])\n",
    "    num_style_imgs = len([name for name in os.listdir(style_img_dir) if os.path.isfile(os.path.join(style_img_dir, name))])\n",
    "    # Divide the style images equally among the input frames:\n",
    "    frames_with_current_style = num_content_imgs // (num_style_imgs - 1)\n",
    "\n",
    "    # Since there are 249 frames in the test video, you can use for loop, \n",
    "    # but prefer using while to avoid using magic numbers (hardcoding)\n",
    "    while(content_image_count <= num_content_imgs):\n",
    "    #for i in range(1, 250):\n",
    "        # Load in the content and style images and move them to the GPU if available:\n",
    "        content_image = load_image(content_img_dir + '/frame_' + str(content_image_count) + '.jpg').to(device)\n",
    "        if(content_image_count % frames_with_current_style == 0):\n",
    "            style_image_count += 1\n",
    "\n",
    "        style_image = load_image(style_img_dir + '/style_' + str(style_image_count) + '.jpg', shape=content_image.shape[-2:]).to(device)\n",
    "\n",
    "        # Retrieve the features:\n",
    "        content_features = get_features(content_image, styleTransferModel)\n",
    "        style_features = get_features(style_image, styleTransferModel)\n",
    "\n",
    "        # Calculate the gram matrix for each of our style representations:\n",
    "        style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "        # We create a 'target image'. Note that, we are starting with the content image\n",
    "        # and cloning it instead of creating an image with white filter:\n",
    "        # We want to update our image based on the total loss and so we will turn on the gradients:\n",
    "        target_image = content_image.clone().requires_grad_(True).to(device)\n",
    "\n",
    "        # LOSS AND WEIGHTS:\n",
    "        # We assign weights for each style layer. Weighting earlier layers more will result in *larger* style artifacts:\n",
    "        # Notice we are excluding `conv4_2`, i.e. our content representation:\n",
    "        style_weights = {'conv1_1': 1.,  # More style will come from earlier layers as they are weighted more\n",
    "                         'conv2_1': 0.8,\n",
    "                         'conv3_1': 0.2,\n",
    "                         'conv4_1': 0.2,\n",
    "                         'conv5_1': 0.2}  # Less style from later layers\n",
    "\n",
    "        content_weight = 1  # alpha\n",
    "        style_weight = 1e8  # beta, 1e6 = 1000000.0\n",
    "\n",
    "        # Iteration hyperparameters:\n",
    "        # Update the target image (as we update the model.parameters() in the classifiers):\n",
    "        optimizer = optim.Adam([target_image], lr=0.003)\n",
    "        # Number of iterations to update your image:\n",
    "        steps = 2000\n",
    "        show_every = 1000\n",
    "        for ii in range(1, steps+1):\n",
    "            # Get the features from the target image:\n",
    "            target_features = get_features(target_image, styleTransferModel)\n",
    "            \n",
    "            # 1. Calculate the content loss:\n",
    "            content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
    "            \n",
    "            # 2. Calculate the style loss:\n",
    "            style_loss = 0\n",
    "            # iterate through each style layer and add to the style loss:\n",
    "            for layer in style_weights:\n",
    "                # Get the \"target\" style representation for the layer:\n",
    "                target_feature = target_features[layer]\n",
    "                batch_size, depth, height, width = target_feature.shape\n",
    "\n",
    "                # Calculate the target gram matrix:\n",
    "                target_gram = gram_matrix(target_feature)\n",
    "\n",
    "                # Get the \"style\" from the style gram matrices computed earlier:\n",
    "                style_gram = style_grams[layer]\n",
    "\n",
    "                # the style loss for one layer, weighted appropriately:\n",
    "                layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
    "\n",
    "                # Add to the style loss:\n",
    "                style_loss += layer_style_loss / (depth * height * width)\n",
    "\n",
    "            # Calculate the total loss:\n",
    "            total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "            # Update the target image:\n",
    "            optimizer.zero_grad()  # zero out the gradients from previous iterations\n",
    "            total_loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the target image\n",
    "\n",
    "            # Display the intermediate results if required:\n",
    "            #if ii % show_every == 0:\n",
    "                #print(\"Total loss: \", total_loss.item())\n",
    "                #plt.imshow(img_convert(target_image))\n",
    "                #plt.show()\n",
    "\n",
    "        # Save the style transferred image:\n",
    "        target_image = img_convert(target_image)\n",
    "        plt.imsave('output_style_transferred_frames/st_frame_%d.jpg' % content_image_count, target_image)\n",
    "        print(\"completed style transfer on image: \", content_image_count)\n",
    "        content_image_count += 1\n",
    "        if(content_image_count <= 250):\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7JC9XscOWhD"
   },
   "outputs": [],
   "source": [
    "# Apply style transfer on the input frames:\n",
    "apply_style_transfer('input_content_frames', 'input_style_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgqJvGuCOZML"
   },
   "outputs": [],
   "source": [
    "# Convert the final images to video:\n",
    "def img_to_video(st_output_dir):\n",
    "    img = cv2.imread(st_output_dir + '/st_frame_1.jpg')\n",
    "    height, width, layers = img.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter('output_processed_video/style_transfered_video.avi', fourcc, 25, (width, height))\n",
    "    for i in range(1, 250):\n",
    "        video.write(cv2.imread(st_output_dir + '/st_frame_' + str(i) + '.jpg'))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "img_to_video('output_style_transferred_frames')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Video_Artistic_Style_Transfer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
